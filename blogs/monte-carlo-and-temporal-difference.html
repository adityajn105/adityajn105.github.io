<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Monte Carlo and Temporal Difference Learning Methods | Blogs | Aditya Jain</title>

    <meta name="description" content="Monte Carlo and Temporal Difference learning in Unknown Environments"/>
    <meta name="keywords" content="adityajn105, aditya, jain, data science, reinforcement learning, monte carlo methods, temporal difference" />
    <meta name="author" content="Aditya Jain" />
    <!-- Facebook and Twitter integration -->
    <meta property="og:title" content="Monte Carlo and Temporal Difference learning in Unknown Environments | Blog by Aditya Jain"/>
    <meta property="og:image" content="../favicon.ico"/>
    <meta property="og:url" content="blogs/monte-carlo-and-temporal-difference.html"/>
    <meta property="og:site_name" content="Aditya Jain | Portfolio"/>
    <meta property="og:description" content="Monte Carlo and Temporal Difference learning in Unknown Environments"/>
    <meta name="twitter:title" content="Monte Carlo and Temporal Difference learning in Unknown Environments | Blog by Aditya Jain" />
    <meta name="twitter:image" content="../favicon.ico" />
    <meta name="twitter:url" content="blogs/monte-carlo-and-temporal-difference.html" />

    <!-- favicon -->
    <link href="../favicon.ico" rel=icon>
    <!-- web-fonts -->
    <link href="https://fonts.googleapis.com/css?family=Hind:300,400,500,600,700" rel="stylesheet">
    <!-- font-awesome -->
    <link href="../css/font-awesome.min.css" rel="stylesheet">
    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <!-- Style CSS -->
    <link href="style.css" rel="stylesheet">
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123324949-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123324949-1');
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <noscript>
        Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
    </noscript>

</head>
<body id="page-top" data-spy="scroll" data-target=".navbar">
<div id="main-wrapper">
<!-- Page Preloader -->
<div id="preloader">
    <div id="status">
        <div class="status-mes"></div>
    </div>
</div>

<div class="columns-block container">
<div class="left-col-block blocks">
    <header class="header theiaStickySidebar">
        <h1></h1>
        <div class="content">
            <span class="lead">10 April 2019 | Aditya Jain</span>
            <h2>Monte Carlo and Temporal Difference Learning Methods</h2>

            <h3>Contents</h3>
            <ul>
                 <li><b>Exploitation vs Exploration</b></li>
                 <li><b>Monte Carlo Methods</b></li>
                 <li>1. First Visit Monte-Carlo Control</li>
                 <li>2. Every Visit Monte-Carlo Control</li>
                 <li><b>Temporal Difference Learning</b></li>
                 <li>1. SARSA (On policy TD control)</li>
                 <li>2. Q Learning (Off policy TD control) </li>
            </ul>

            <ul class="social-icon">
                <li><a href="https://github.com/adityajn105"><i class="fa fa-github fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://linkedin.com/in/adityajn105"><i class="fa fa-linkedin fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://www.instagram.com/adityajn105/"><i class="fa fa-instagram fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://twitter.com/adityajn105"><i class="fa fa-twitter fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://www.facebook.com/adityajn105"><i class="fa fa-facebook fa-2x" aria-hidden="true"></i></a></li>
            </ul>

            <a href="/" class="btn btn-primary">Go Back</a>
        </div>

    </header>
    <!-- .header-->
</div>
<!-- .left-col-block -->


<div class="right-col-block blocks">
<div class="theiaStickySidebar">
<section class="section-wrapper section-interest gray-bg">
    <div class="container-fluid">
        <div class="col-md-12">
                <p>
                Before we go ahead and start discussing about monte carlo and temporal difference learning for policy optimization, I think you must have knowledge about the
                policy optimization in known environment i.e. Value Iteraions and Policy Iterations. My advice is to check out the below article to learn about Markov Decision Process,
                Value Iterations and Policy Iterations.
                <br><br>
                <a href='/blogs/policy-optimization-rl.html'>Policy Optimization in Reinforcement Learning.</a>
                <br><br>
                Policy Iterations and Value Iterations are algorithms based on dynamic programming which requires knowledge of complete MDP where all environment variables are known.
                Here the goal of agent was to learn a policy. This problem is called as a Model-Based Learning.
                <br><br>
                But in real scenerios we do not have knowledge of environment and we have to learn about the environment by experimenting or interacting with the environment.
                This problem is called Model Free Learning.
                <br><br>
                1. We will miss a transition model, so we don't have know what's going to happen after each action we take before hand.
                <br>
                2. We will also miss our reward function.
                </p>
                <h3>Exploitation vs Exploration</h3>
                <p>
                This applies to system that want to aquire new knwoledge and maximize their reward at the same time.
                <br>
                Example. Think of a slot machine in casino. Each machine has its own probability of winning. As a player you want to make as much money as possible. Here's the dilemma.
                How do you figure out which of the machine have best odds, while at the same time maximizing your profit? If you constantly played one of the machines you would never gain any knwoledge about
                the odds of other machine. Also if you always picked a machine at random you would learn a lot about other machines at random you would learn a lot about other machines bit probably wouldn't make
                as much as money as could have always playing "best machine".
                <br><br>
                This applies to principle of our choices in lives. Do we try drastically different things to explore that what makes us happy, or do we exploit our current situation and knowledge to make the best out of it?
                In today's society we are loosely based on what is called epsilon-decreasing startegy. Epsilon decreases over time, we try to figure out what we want to do at young age, and then stick with it throughout our lives.
                Is this strategy optimal? or it is imposed on us by the society.
                <br>
                In slot machine example we wanted to make as much money as possible. This was matrix we tried to optimize. In real life example matrix is how much money we make? How much free time we have? How safe family is?
                <br><br>
                Let's look at some strategies which can help us with this dilemma.
                <h4>1. Epsilon-decreasing with softmax</h4>
                <p>We exploit the current situation with the probability 1-epsilon and explore a new option with the probability epsilon, with epsilon decreasing over time. In case of exploring we dont't want to just pick
                each option at random, instead we estimate the outcome of each option and then pick based on that (softmax part).</p>
                <h4>2. Upper confidence bound strategy</h4>
                <p>We always pick the option with highest possible outcome, even if that outcome is very unlikely. The intuition behind this stategy is that options with high uncertainity unsually lead to a lot of new knowledge.
                We don't know enough about option to accurately estimate the return and by pursuing the option we are bound to learn more and improve our future estimation. In simulated settings this algorithm does well when we have many
                options with differet variances.</p>
                <h4>3. Contextual-Epsilon-greedy strategy</h4>
                <p>Similar to epsilon-greedy, but we choose the value of epsilon based on how critical our situation is. When we are in critical situation (large debt, need to provide for a sick family) we always exploit instead of explore-we do what we know works well.
                If we are in situation that is not critical we are more likely to explore new things. This strategy makes intuitive sense, but I believe that is not commonly followed. Even in non-critical situations we often choose to keep doing what we have always done
                due to our risk-averse nature.</p>
                <h4>4. Optimistic Initialization</h4>
                <p>This is an simple and practical idea. Initialize everything as everything is highly rewarding. We will assume that a action in a state is highly rewarding untill proven otherwise. This methods gives chance to every state-action pair to
                prove itselt. But it could take a lot of time since we now need to try and explore every state-action pair to know its exact value.</p>
                <h4>5. Optimism in the face of uncertainity</h4>
                In this we will always pick the action which have a chance of being highly rewarding. Let's understand this with a simple example below.
                <img src="images/optimisim_in_the_face_of_certainity.png" class="img-responsive">
                <br>
                Here, what action should we pick? The more uncertain we are about an action value. The more important is to explore an action. It could turn out to be the best action.
                <br> Here we will pick the blue action as it seems to be highly rewarding however it very less unlikely that red action.
              </p>
              <h2>Monte Carlo Methods : </h2>
              <p>Monte Carlo methods are large family of compuational algorithms that rely on random sampling. These methods are mainly used for
              numerical integration, stochastic optimization, character distributions etc.
              <br><br>
              Monte Carlo vs Dynamic Programming:
              <br>1. No Need of Complete Markov Decision process.
              <br>2. Computatinally More efficient.
              <br>3. Can be used with stochastic simulators.
              <br><br>
              In reinforcement learning for a unknown MDP environment or say Model Free Learning. Monte Carlo will learn directly from the epsiode of experience. MC uses simplest possible idea
              value of a state-action pair is mean of all returns.
              <br><b>Goal: </b>Learn `V_{\pi}` from episode of experience.
              <br><b>Return: </b> Return is total discounted reward ` G = R_{t+1} + \gamma R_{t+2} +....+ \gamma^{t-1}R_{T} `
              <br><b>Value Function: </b> Value function is expected return ` V_{\pi}(s) = E_{\pi} [ G_{t} | S_{t}=S ] `
              <br><br><b>MC uses emperical mean return instead of expected return.</b>
              <br><br>
              There are 2 types of Monte Carlo Control Methods
              <br>1. On-policy First Visit Monte Carlo Control : In each episode, we will consider only first visit of every state-action pair to calculate the mean return.
              <br>2. On-policy Every Visit Monte Carlo Control : In each episode, we will consider every visit of every state-action pair to calucalte the mean return.
              Here we will discuss First Visit Monte Carlo Control only.
              </p>
              <h4>Algorithm for First Visit Monte Carlo Control:</h4>
              <ul>
                <li>Initialize for all s `\in`, a `\in` A(s):</li>
                <li>
                  <ul>
                    <li>  Q(s,a) `\leftarrow` arbitary  </li>
                    <li>  Visit(s,a) `\leftarrow` 0  </li>
                    <li> `\pi`(a|s) `\leftarrow` an arbitary `\epsilon` soft policy  </li>
                  </ul>
                </li>
                <li>Repeat Forever</li>
                <li>
                  <ul> Generate an episode using `\pi` </li> </ul>
                  <ul> For each pair s,a appearing in the episode
                    <ul>
                      <li>  G `\leftarrow` the return the follows first occurence of s,a  </li>
                      <li>  Visit(s,a) `\leftarrow` Visit(s,a) + 1 </li>
                      <li> Q(s,a) `\leftarrow` Q(s,a) + 1/Visit(s,a) (G - Q(s,a)) #this is the incremental mean</li>
                    </ul>
                  </ul>
                </li>
                <li>
                  <ul> For each S in episode
                    <ul>
                      <li> `A^\ast \leftarrow argmax_a Q(s,a) `</li>
                      <li> For all `a \in A(s)`:<br>
                            $$ \begin{equation}
                                  \pi(a,s) = \left \{
                                    \begin{aligned}
                                    &max(A^\ast(s,a)), && \text{if } \text{random} > \epsilon \\
                                    &\text{random}(A(s)), && \text{otherwise }
                                    \end{aligned} \right.
                                \end{equation} $$
                      </li>
                    </ul>
                  </ul>
                </li>
              </ul>

              <h4>Code for First Visit Monte Carlo Control to solve Frozen Lake OpenAI gym game:</h4>
              <script src="https://gitlab.com/snippets/1846123.js"></script>

      <h2>Temporal Difference Learning</h2>
            <p>
              It is a combination of Monte Carlo Learning and Dynamic Programming. Just like Monte Carlo, Temporal Difference method also learn directly from the episodes of experience.
              But in contrast to Monte Carlo Learning, Temporal Difference learning will not wait till the end of episode to update expected future rewards estimation(V), it will wait only until the next time
              step to update value estimates.
              <br><br>
              In Monte-carlo: $$ V(S_t) \leftarrow V(S_t) + \alpha \Big( G_t  - V(S_t) \Big) $$
              <br>
              In Temporal Difference: $$ V(S_t) \leftarrow  V(S_t) + \alpha \Big( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \Big) $$
              <br>
              Where,
              <br> ` R_{t+1} + \gamma V(S_{t+1}) ` is estimated return or TD target
              <br> ` \partial_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) ` is called TD error
              <br><br>

              Temporal Difference is generally more effective thatn Monte Carlo. It is more sensitive to intial value. Temporal Difference Control Learning has 2 alogrithm.
              <br>
              <b>1. SARSA (On policy TD control)</b>
              <br>
              <b>2. Q Learning (Off policy TD control)</b>
              <br><br>
              Let's discuss On-policy learning and Off-policy learning before going into these algorithms.
              <br>
              <b>On-policy Learning : </b>  On policy learning method , means it uses the same policy to choose the next action A'. Learn about the policy `\pi` from the experience sampled from `\pi`.
              <br>
              <b>Off-policy Learning : </b> Off policy learning method , means, it uses the target policy (greedy) to choose the best next action A' while following the behavior policy (epsilon-greedy).
              <br>
              </p>
              <h3>SARSA (state-action-reward-state-action)</h3>
              <p> $$ Q(S,A) \leftarrow Q(S,A) + \alpha \Big( R + \gamma Q(S',A') - Q(S,A) \Big) $$
                <br>
                The agent starts in state S, performs action A, and get reward R and goes to state S' and chooses action A' there and then update the value of A' in S.
                Here TD target is ` R + \gamma Q(S',A') ` and TD error is ` R + \gamma Q(S',A') - Q(S,A)  `
                <br>
              </p>
              <h4>Algorithm for SARSA : An On-policy TD control Algorithm : </h4>
              <ul>
                <li> Initialize `Q(s,a) \forall s \in S, a \in A(S) arbitarily and Q`(terminal) = 0 </li>
                <li> Repeat (for each episode): </li>
                <li>
                  <ul>
                    <li> Initialize S </li>
                    <li> Choose A from S using policy derived from `Q(\epsilon-greedy )`</li>
                    <li> Repeat(for each step of episode)</li>
                    <li>
                      <ul>
                        <li> Take action A, observe R, S' </li>
                        <li> Choose A' from S' using policy derived from `Q(\epsilon-greedy)`.</li>
                        <li> `Q(S,A) \leftarrow Q(S,A) + \alpha [ R + \gamma Q(S',A') - Q(S,A) ] `</li>
                        <li> ` S \leftarrow S', A \leftarrow A' ` </li>
                      </ul>
                    </li>
                    <li> Until S is terminated </li>
                  </ul>
                </li>
              </ul>
              <br>
              <h3>Q-Learning</h3>
              <p>
                One of the most important breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning.
                Q-learning estimates a state-action value function for a target policy that deterministically selects the action of highest value.
                $$ Q(S,A) \leftarrow Q(S,A) + \alpha \Big( R + \gamma \max_{a'} Q(S',A') - Q(S,A) \Big) $$
                Here, TD target is `R + \gamma \max_{a'} Q(S',A')` and TD error is `R + \gamma \max_{a'} Q(S',A') - Q(S,A)`
                <br>
                <b>Why Q-Learning?</b>
                <br>1. Reuse experience generated form old policies ` \pi_1, \pi_2, \pi_3, ..... `
                <br>2. Learn about optimal policy while following exploratory policy.
                <br>3. Learn about multiple policy while following one policy.
                <br>4. Learn about observing human or other agents.
              </p>
              <h4>Algorithm for Q-Learning : An Off-policy TD control Algorithm : </h4>
              <ul>
                <li> Initialize `Q(s,a) \forall s \in S, a \in A(S) arbitarily and Q`(terminal) = 0 </li>
                <li> Repeat (for each episode): </li>
                <li>
                  <ul>
                    <li> Initialize S </li>
                    <li> Choose A from S using policy derived from `Q(\epsilon-greedy )`</li>
                    <li> Repeat(for each step of episode)</li>
                    <li>
                      <ul>
                        <li> Take action A, observe R, S' </li>
                        <li> `Q(S,A) \leftarrow Q(S,A) + \alpha [ R + \gamma \max_{a'} Q(S',A') - Q(S,A) ] `</li>
                        <li> ` S \leftarrow S' ` </li>
                      </ul>
                    </li>
                    <li> Until S is terminated </li>
                  </ul>
                </li>
              </ul>
              <h4>Code for Q-Learning to solve Frozen-Lake OpenAI gym game:</h4>
              <script src="https://gitlab.com/snippets/1846198.js"></script>

            <h2> Whats's Next?</h2>
            <p>
              In <a href='/blogs/deep-q-learning.html'>next blog</a>, I am going to discuss about limitations of Q-Learning and
              will come up with the solution to those limitaitons that is Deep Q Learning. Also we will discuss some more advance technique to improve our Deep Q algorithm.
              Also we will build a reinforcement learning agent to play flappy bird game.
            </p>
            <h2>More Resources</h2>
                <ol>
                    <li><a href="https://medium.com/deep-math-machine-learning-ai/ch-12-1-model-free-reinforcement-learning-algorithms-monte-carlo-sarsa-q-learning-65267cb8d1b4" target="_blank">Model Free Reinforcement Learning Algorithm</a></li>
                    <li><a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT" target="_blank">RL Course of David Silver - Lecture</a></li>
                    <li><a href="http://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" target="_blank">Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto</a></li>
                    <li><a href="https://github.com/adityajn105/Move37/tree/master/Classroom-Codes" target="_blank">Example codes and problems to understand concepts better.</a></li>
                </ol>
        </div>
        <script>
            var disqus_config = function () {
            this.page.url = "";  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = 4; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
            };
            (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://adityajn105.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
            })();
        </script>
        <div id="disqus_thread"></div>      
    </div>
</section>
<section class="section-contact section-wrapper gray-bg">
    <div class="container-fluid">
        <div class="row">
            <div class="col-md-12">
                <div class="section-title">
                    <h2>Contact</h2>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <div class="feedback-form">
                    <h2>Get in touch</h2>

                    <form id="contactForm" action="https://formspree.io/adityajn105@gmail.com" method="POST" target='_blank'>
                        <div class="form-group">
                            <input type="hidden" name="_subject" value="Contact request from personal website" 
                            class="form-control" id="InputSubject">
                        </div>
                        <div class="form-group">
                            <label for="InputEmail">Email address</label>
                            <input type="email" name="_replyto" required="" class="form-control" id="InputEmail"
                                   placeholder="Email">
                        </div>
                        <div class="form-group">
                            <label for="message-text" class="control-label">Message</label>
                            <textarea class="form-control" rows="4" required="" name="message" id="message-text"
                                      placeholder="Write message"></textarea>
                        </div>

                        <button type="submit" class="btn btn-primary">Submit</button>
                    </form>
                </div>
            </div>
        </div>
    </div>
    <!--.container-fluid-->
</section>

<footer class="footer">
    <div class="copyright-section">
        <div class="container-fluid">
            <div class="row">
                <div class="col-md-12">
                    <div class="copytext">&copy; Resumex. All rights reserved | Design By: <a
                            href="https://themehippo.com">themehippo</a></div>
                </div>
            </div>
            <!--.row-->
        </div>
        <!-- .container-fluid -->
    </div>
    <!-- .copyright-section -->
</footer>
<!-- .footer -->
</div>
<!-- Sticky -->
</div>
<!-- .right-col-block -->
</div>
<!-- .columns-block -->
</div>
<!-- #main-wrapper -->

<!-- jquery -->
<script src="../js/jquery-2.1.4.min.js"></script>
<!-- Bootstrap -->
<script src="../js/bootstrap.min.js"></script>
<script src="../js/theia-sticky-sidebar.js"></script>
<script src="../js/scripts.js"></script>
</body>
</html>