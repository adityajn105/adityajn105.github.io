<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
     <title>Attention Model in Machine Translation | Blogs | Aditya Jain</title>

    <meta name="description" content="Attention Model in Machine Translation"/>
	<meta name="keywords" content="adityajn105, aditya, jain, artificial intelligence, machine translation" />
	<meta name="author" content="Aditya Jain" />
	<!-- Facebook and Twitter integration -->
	<meta property="og:title" content="Attention Model in Machine Translation | Blog by Aditya Jain"/>
	<meta property="og:image" content="../favicon.ico"/>
	<meta property="og:url" content="blogs/attention-model.html"/>
	<meta property="og:site_name" content="Aditya Jain | Portfolio"/>
	<meta property="og:description" content="Attention Model in Machine Translation"/>
	<meta name="twitter:title" content="Attention Model in Machine Translation | Blog by Aditya Jain" />
	<meta name="twitter:image" content="../favicon.ico" />
	<meta name="twitter:url" content="blogs/attention-model.html" />


    <!-- favicon -->
    <link href="../favicon.ico" rel=icon>
    <!-- web-fonts -->
    <link href="https://fonts.googleapis.com/css?family=Hind:300,400,500,600,700" rel="stylesheet">
    <!-- font-awesome -->
    <script src="https://kit.fontawesome.com/9aa077278c.js" crossorigin="anonymous"></script>
    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <!-- Style CSS -->
    <link href="style.css" rel="stylesheet">
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123324949-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123324949-1');
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <noscript>
        Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
    </noscript>

</head>
<body id="page-top" data-spy="scroll" data-target=".navbar">
<div id="main-wrapper">
<!-- Page Preloader -->
<div id="preloader">
    <div id="status">
        <div class="status-mes"></div>
    </div>
</div>

<div class="columns-block container">
<div class="left-col-block blocks">
    <header class="header theiaStickySidebar">
        <h1></h1>
        <div class="content">
            <span class="lead">10 Jun 2019 | Aditya Jain</span>
            <h2>Attention Model for Machine Translation</h2>

            <h3>Contents</h3>
            <ul>
                 <li>Introduction</li>
                 <li><b>Attention Model</b></li>
                 <li><b>Attention Mechanism</b></li>
                 <li>Building a Date Translater</li>
                 <li>More Resources</li>
            </ul>

            <ul class="social-icon">
                <li><a href="https://github.com/adityajn105" target="_blank"><i class="fab fa-github fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://linkedin.com/in/adityajn105" target="_blank"><i class="fab fa-linkedin fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://hub.docker.com/u/adityajn105" target="_blank"><i class="fab fa-docker fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://www.instagram.com/adityajn105/" target="_blank"><i class="fab fa-instagram fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://twitter.com/adityajn105" target="_blank"><i class="fab fa-twitter fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://www.facebook.com/adityajn105" target="_blank"><i class="fab fa-facebook fa-2x" aria-hidden="true"></i></a></li>

            </ul>

            <a href="/" class="btn btn-primary">Go Back</a>
        </div>

    </header>
    <!-- .header-->
</div>
<!-- .left-col-block -->


<div class="right-col-block blocks">
<div class="theiaStickySidebar">
<section class="section-wrapper section-interest gray-bg">
    <div class="container-fluid">
        <div class="col-md-12">
                <p>
                		<h2>Introduction</h2>
			    		Machine translation (MT) refers to fully automated software that can translate source content into target content of different type. Humans may use MT to help them render text and speech into another language, or the MT software may operate without human intervention. Neural Machine Translation is method which utilizes neural networks to achieve this task.
			    		<br><br>
			    		Suppose if you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. That's the main idea behind attention model. The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. Attention model is one of the most sophisticated sequence to sequence models.
			    		<br><br>
			    		The material presented here is taken from the Deep Learning Specialization Course by Andrew Ng for the sake of explanation.
			    		<br><br>
			    		<h2>Attention Model</h2>
			            The diagram below shows the whole attention model in one view.
			            <br>
			            <div class="col-md-12"><img src="images/attn_model.png" class="col-md-push-2 col-md-8"></div>
			            <br><br><br>
			            There are two seperate LSTMs in this model. The one in the bottom is the Bi-Direction LSTM and comes into play before the attention mechanism, let's call it pre-attention Bi-LSTM. The LSTM on the top comes after the attention mechanism and let's call it post-attention LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps.
			            <br><br>
			            The post-attention LSTM passes $s^{\langle t \rangle}, c^{\langle t \rangle}$ from one time step to the next. LSTM has both the output activation $s^{\langle t\rangle}$ and the hidden cell state $c^{\langle t\rangle}$. In this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\langle t-1 \rangle}$ as input; it only takes $s^{\langle t\rangle}$ and $c^{\langle t\rangle}$ as input because here we will build model for date generation, because (unlike language generation where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date.
			            <br><br>
			            We use $a^{\langle t \rangle} = [\overrightarrow{a}^{\langle t \rangle}; \overleftarrow{a}^{\langle t \rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM. 

			            <br><br>
			            <h2>Attention Mechanism</h2>
			            The diagram below shows what one "Attention" step does to calculate the attention variables $\alpha^{\langle t, t' \rangle}$, which are used to compute the context variable $context^{\langle t \rangle}$ for each timestep in the output ($t=1, \ldots, T_y$).
			            <br><br>
			            <div class="col-md-12"><img src="images/attn_mechanism.png" class="col-md-push-2 col-md-8"></div> 
			           	<br>
			           	The diagram above uses a RepeatVector node to copy $s^{\langle t-1 \rangle}$'s value $T_x$ times, and then Concatenation to concatenate $s^{\langle t-1 \rangle}$ and $a^{\langle t \rangle}$ to compute $e^{\langle t, t'}$, which is then passed through a softmax to compute $\alpha^{\langle t, t' \rangle}$.
			           	<br><br>
			           	At step $t$, given all the hidden states of the Bi-LSTM ($[a^{&lt;1&gt;},a^{&lt;2&gt;}, ..., a^{&lt;T_x&gt;}]$) and the previous hidden state of the second LSTM ($s^{&lt;t-1&gt;}$). One step of attention will compute the attention weights ( $[\alpha^{&lt;t,1&gt;}, \alpha^{&lt;t,2&gt;}, ..., \alpha^{&lt;t,T_x&gt;}]$ ) and output the context vector as
			           	$$ context^{\lt t \gt} = \sum_{t'=1}^{T_x} \alpha^{ \lt t,t' \gt } \alpha^{t'}   $$
			           	The (post-attention) LSTM's internal memory cell variable is denoted by $c^{\langle t \rangle}$ not to be confused with $context^{\lt t \gt}$.
			           	<br><br>
			           	<h2>Building a Date Translater</h2>
			           	We will build a Neural Machine Translation (NMT) model to translate human readable dates ("10th of May, 1996") into machine readable dates ("1996-05-10") which I got inspiration from Deep Learning Course of Coursera by Andrew Ng. The model you will build here could be used to translate from one language to another, such as translating from English to Hindi. However, language translation requires massive datasets and usually takes days of training on GPUs.
			           	<br><br>
			           	Here I will talk about building attention model. Faker library is used to generate human readable and machine readable dates dataset, you can refer <a href="https://github.com/adityajn105/Attention-Based-Machine-Translation-Demo/blob/master/Attention_Based_Machine_Translation.ipynb">Github Code</a> to implement that.
			           	<br><br>
			           	Firstly we will implement a one_step_attention() method. Let's say we have hidden states of Bi-LSTM as ($[a^{&lt;1&gt;},a^{&lt;2&gt;}, ..., a^{&lt;T_x&gt;}]$) and previous hidden state of the second LSTM ($s^{&lt;t-1&gt;}$). We will compute $context^{\lt t \gt}$ as follows:
		           		<script src="https://gitlab.com/snippets/1865271.js" type="text/javascript"></script>
		           		Now let's implement model, we will have three input first one is input data, second and third is initial cell state and initial cell memory of post-attention LSTM respectively since the first LSTM cell will not have any inpu in starting. We will use Bi-Directional wrapper around LSTM which will concat two hidden activation of LSTM cell as $a^{\langle t \rangle} = [\overrightarrow{a}^{\langle t \rangle}; \overleftarrow{a}^{\langle t \rangle}]$.
		           		<br><br>
		           		We have to make out post-attention LSTM cell which make its return_state as True. The post attention LSTM cell will return state and memory where its state is used to calculate context using one_step_attention which will be input for next post-attention LSTM cell. Output is generated by applyting dense with softmax activation on LSTM's hidden state output.
		           		<br><br>
		           		Now, we are ready to define out model.
		           		<script src="https://gitlab.com/snippets/1865273.js" type="text/javascript"></script>
		           		Now its time to train oue model. We will initialize initial cell state and memory as array of zeros. We are using Adam Optimizer which some hyperparameter which turn out to be working best. 
		           		<br><br>
		           		Since output is 10 dimensional. So we need to change the shape of out training data output. Finally we are ready to start training.  
		           		<script src="https://gitlab.com/snippets/1865278.js" type="text/javascript"></script>
		         		Some examples translated by our model:
		         		<br><br>
		         		3 May 1979 -> 1979-05-03<br>
						5 April 09 -> 2019-05-00<br>
						21th of August 2016 -> 2016-08-21<br>
						Tue 10 Jul 2007 -> 2007-07-10<br>
						Saturday May 9 2018 -> 2018-05-09<br>
						March 3 2001 -> 2001-03-03<br>
						March 3rd 2001 -> 2001-03-03<br>
						1 March 2001 -> 2001-03-01<br>
						jun 10 2017 -> 2017-06-10

		         		<br><br>
		         		So we are ready with our own date translation application using one of the "state of the art" technique. I think you are ready to build your own machine translation application. Feel free to mention its link in the comments.
		         		<br>
		         		<h3>Here is <a href="https://github.com/adityajn105/Attention-Based-Machine-Translation-Demo/blob/master/Attention_Based_Machine_Translation.ipynb">Github Link</a> for full Code</h3>
		         		<br>
						<h2>More Resources</h2>
			            <ol>
			                <li><a href="https://machinelearningmastery.com/introduction-neural-machine-translation/" target="_blank">Introduction to neural machine translation.</a></li>
			                <li><a href="https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/" target="_blank">Buiding a German to English language translater.</a></li>
			                <li><a href="https://google.github.io/seq2seq/nmt/" target="_blank">Neural Machine Translation Background</a></li>
			            </ol>
        </div>
        <script>
            var disqus_config = function () {
            this.page.url = "";  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = 8; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
            };
            (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://adityajn105.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
            })();
        </script>
        <div id="disqus_thread"></div>      
    </div>
</section>
<section class="section-contact section-wrapper gray-bg">
    <div class="container-fluid">
        <div class="row">
            <div class="col-md-12">
                <div class="section-title">
                    <h2>Contact</h2>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <div class="feedback-form">
                    <h2>Get in touch</h2>

                    <form id="contactForm" action="https://formspree.io/adityajn105@gmail.com" method="POST" target='_blank'>
                        <div class="form-group">
                            <input type="hidden" name="_subject" value="Contact request from personal website" 
                            class="form-control" id="InputSubject">
                        </div>
                        <div class="form-group">
                            <label for="InputEmail">Email address</label>
                            <input type="email" name="_replyto" required="" class="form-control" id="InputEmail"
                                   placeholder="Email">
                        </div>
                        <div class="form-group">
                            <label for="message-text" class="control-label">Message</label>
                            <textarea class="form-control" rows="4" required="" name="message" id="message-text"
                                      placeholder="Write message"></textarea>
                        </div>

                        <button type="submit" class="btn btn-primary">Submit</button>
                    </form>
                </div>
            </div>
        </div>
    </div>
    <!--.container-fluid-->
</section>

<footer class="footer">
    <div class="copyright-section">
        <div class="container-fluid">
            <div class="row">
                <div class="col-md-12">
                    <div class="copytext">&copy; Resumex. All rights reserved | Design By: <a
                            href="https://themehippo.com">themehippo</a></div>
                </div>
            </div>
            <!--.row-->
        </div>
        <!-- .container-fluid -->
    </div>
    <!-- .copyright-section -->
</footer>
<!-- .footer -->
</div>
<!-- Sticky -->
</div>
<!-- .right-col-block -->
</div>
<!-- .columns-block -->
</div>
<!-- #main-wrapper -->

<!-- jquery -->
<script src="../js/jquery-2.1.4.min.js"></script>
<!-- Bootstrap -->
<script src="../js/bootstrap.min.js"></script>
<script src="../js/theia-sticky-sidebar.js"></script>
<script src="../js/scripts.js"></script>
</body>
</html>