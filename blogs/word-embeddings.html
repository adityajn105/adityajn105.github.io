<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Aditya Jain |  Blogs | Word Embeddings</title>

    <meta name="description" content="In NLP application, we have to convert word into some sort of numerical representation. "/>
    <meta name="keywords" content="adityajn105, aditya, jain, data science, deep learning, machine learning, nlp, android" />
    <meta name="author" content="Aditya Jain" />

    <meta property="og:title" content="Aditya Jain | Word Embeddings"/>
	<meta property="og:image" content="../favicon.ico"/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content="Aditya Jain | Word Embeddings"/>
	<meta property="og:description" content="In NLP application, we have to convert word into some sort of numerical representation. "/>
	<meta name="twitter:title" content="Aditya Jain | Portfolio | Projects | Blogs" />
	<meta name="twitter:image" content="../favicon.ico" />
	<meta name="twitter:url" content="" />

    <!-- favicon -->
    <link href="../favicon.ico" rel=icon>
    <!-- web-fonts -->
    <link href="https://fonts.googleapis.com/css?family=Hind:300,400,500,600,700" rel="stylesheet">
    <!-- font-awesome -->
    <script src="https://kit.fontawesome.com/9aa077278c.js" crossorigin="anonymous"></script>
    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <!-- Style CSS -->
    <link href="style.css" rel="stylesheet">
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123324949-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-123324949-1');
	</script>
	<script>
		var disqus_config = function () {
		this.page.url = "https://adityajn105.github.io/blogs/word-embeddings.html";  // Replace PAGE_URL with your page's canonical URL variable
		this.page.identifier = 1; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
		};
		(function() { // DON'T EDIT BELOW THIS LINE
		var d = document, s = d.createElement('script');
		s.src = 'https://adityajn105.disqus.com/embed.js';
		s.setAttribute('data-timestamp', +new Date());
		(d.head || d.body).appendChild(s);
		})();
	</script>
	<noscript>
		Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
	</noscript>

</head>
<body id="page-top" data-spy="scroll" data-target=".navbar">
<div id="main-wrapper">
<!-- Page Preloader -->
<div id="preloader">
    <div id="status">
        <div class="status-mes"></div>
    </div>
</div>

<div class="columns-block container">
<div class="left-col-block blocks">
    <header class="header theiaStickySidebar">
    	<h1></h1>
        <div class="content">
            <span class="lead">10 September 2018 | Aditya Jain</span>
            <h2>Word Embeddings in Natural Language Processing</h2>

			<h3>Contents</h3>
            <ul>
            	 <li><b>1. Frequency based Embedding</b></li>
            	 <li>a. Count Vector</li>
            	 <li>b. TfIdf Vector</li>
            	 <li><b>2. Prediction Based Vector</b></li>
            	 <li>a. CBOW (Continuous Bag of Words)</li>
            	 <li>b. Skip Gram</li>
            </ul>

           <ul class="social-icon">
            	<li><a href="https://github.com/adityajn105" target="_blank"><i class="fab fa-github fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://linkedin.com/in/adityajn105" target="_blank"><i class="fab fa-linkedin fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://hub.docker.com/u/adityajn105" target="_blank"><i class="fab fa-docker fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://www.instagram.com/adityajn105/" target="_blank"><i class="fab fa-instagram fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://twitter.com/adityajn105" target="_blank"><i class="fab fa-twitter fa-2x" aria-hidden="true"></i></a></li>
                <li><a href="https://www.facebook.com/adityajn105" target="_blank"><i class="fab fa-facebook fa-2x" aria-hidden="true"></i></a></li>

            </ul>

            <a href="/" class="btn btn-primary">Go Back</a>
        </div>

    </header>
    <!-- .header-->
</div>
<!-- .left-col-block -->


<div class="right-col-block blocks">
<div class="theiaStickySidebar">
<section class="section-wrapper section-interest gray-bg">
    <div class="container-fluid">
    	<div class="col-md-12">
	    	<p>
	    		In NLP application we have to work with texual data. 
				Well we can't directly feed our textual data for training into our ML models, Deep Learning Models etc.
				Let it be regression, classification or any NLP task, we need to convert our textual data into numerical form that can be feeded into models for futher processing.
			</p>
			<p>
				Word Embedding converts textual data into numerical data of some form. In general, word embedding convert a word into some sort of vector representation.
			</p>
			<p>
				Now, we will broadly classify word embedding in 2 types and then dive deep into their types
				<br>
				<ol>
					<li>
						<h3>Frequency based Embedding</h3>
						<ul>
							<li>Count Vector</li>
							<li>Tf-Idf Vector</li>
						</ul>
					</li>
					<li>
						<h3>Prediction based Embedding</h3>
						<ul>
							<li>CBOW (Continous Bag of Words)</li>
							<li>Skip-Gram</li>
						</ul>
					</li>
				</ol>
			</p>
			<p>
				<h2>1. Frequency based Embedding</h2>
				These are the very basic, easy and fast method to word vectors. These work on the basis of count of word in each document. It can be Count Vector, Tf-Idf vector, or Co-Occurance vector. We will discuss here only Count vector and Tf-Idf vector.

				<h3>1.1  Count Vector</h3>
				Lets us understand this by looking into a simple example. Lets take two documents
				<br><br>
				d1 = "Take a look into the beauty of the word embedding."<br>
				d2 = "The word vectorizer is the most basic word embedding"
				<br><br>
				There are 12 unique words, So here our word vector will be of size 12, which means each word can be denoted by a vector of size 12.
				<br><br>
				Lets, arrange all unique words in alphabetic order. That would be "basic, beauty, embedding, into, is, look, most, take, the, vectorizer, word".
				<br><br>
				Now lets prepare a dictionary where each word is mapped with index in vector.
				<br>
				<code>
				{'basic': 0,'beauty': 1,'embedding': 2,'into': 3,'is': 4,'look': 5,'most': 6,'of': 7,'take': 8,'the': 9,'vectorizer': 10,'word': 11}
				</code>
				<br><br>
				So, suppose we want to denote a word by vector.<br>
				<code>
				vectorizer = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
				</code>
				<br><br>
				Lets convert both our sentence into vector.<br>
				<table>
				    <thead>
				      <tr>
				        <th></th>
				        <th>basic</th>
				        <th>beauty</th>
				        <th>embedding</th>
				        <th>into</th>
				        <th>is</th>
				        <th>look</th>
				        <th>most</th>
				        <th>of</th>
				        <th>take</th>
				        <th>the</th>
				        <th>vectorizer</th>
				        <th>word</th>
				      </tr>
				    </thead>
				    <tbody>
				      <tr>
				        <td>d1</td>
				        <td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td>
				        <td>0</td><td>1</td><td>1</td><td>2</td><td>0</td><td>1</td>
				      </tr>
				      <tr>
				        <td>d1</td>
				        <td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td>
				        <td>1</td><td>0</td><td>0</td><td>2</td><td>1</td><td>2</td>
				      </tr>
				     </tbody>
			  </table>
			  <br>
			  above is given vector representaion of documents d1 and d2
			  <br>
			  Here is the example code, to form count vector using sci-kit library<br>
				<code>
					>>> from sklearn.feature_extraction.text import CountVectorizer<br>
					>>> text = ["Take a look into the beauty of the word embedding.","The word vectorizer is the most basic word embedding"]<br>
					>>> cv = CountVectorizer()<br>
					>>> cv.fit(text)<br>
					>>> text_vector = cv.transform(text)<br>
					>>> text_vector.toarray()<br>
					array([[0, 1, 1, 1, 0, 1, 0, 1, 1, 2, 0, 1],<br>
					       [1, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 2]])<br>
					>>> cv.vocabulary_<br>
					{'basic': 0,<br>
					 'beauty': 1,<br>
					 'embedding': 2,<br>
					 'into': 3,<br>
					 'is': 4,<br>
					 'look': 5,<br>
					 'most': 6,<br>
					 'of': 7,<br>
					 'take': 8,<br>
					 'the': 9,<br>
					 'vectorizer': 10,<br>
					 'word': 11}<br>
				</code>
				<br><br>
				<h3>1.2  TF-IDF Vector</h3>
				It is another method which also based on the frequency of words in documents. But overcome some flaws of countvectorizer. It takes into account not only frequency of word in each document but also entire corpus.
				<br><br>
				Some words like 'the','a','is','that' appear more often then other words in every document. These word doesn't seem to change the sentiment of sentence. So we would like to weight down the words which occur quite often in most of the documents.
				<br><br>
				For documents,
				<br>
				d1 = "Take a look into the beauty of the word embedding."<br>
				d2 = "The word vectorizer is the most basic word embedding"
				<br><br>
				Lets look what Tf-Idf does,
				<br>
				<code>
					TF = ( Freq of word in a document ) / ( No of words in that documents )<br>
					TF(take,d1) = 1/10 = 0.1<br>
					TF(the,d2) = 1/9 = 0.11
					<br><br>
					IDF = log( No of docs /  No of docs term t has appeared ) #without smoothing<br>
					where<br> 
					IDF(the) = log(2/2) = 1<br>
					IDF(take) = log(2/1) = 0.6931<br>
					<br><br>
					TF-IDF(take,d1) = tf*idf = 0.1*0.6931 = 0.0693
				</code>
				<br><br>
				sci-kit learn library uses TF-IDF which by default takes smoothing factor into account so values might be different.
				<br>
				<code>
					>>> from sklearn.feature_extraction.text import CountVectorizer <br>
					>>> text = ["Take a look into the beauty of the word embedding.","The word vectorizer is the most basic word embedding"] <br>
					>>> cv = CountVectorizer()<br> 
					>>> cv.fit(text) <br>
					>>> text_vector = cv.transform(text)<br> 
					>>> text_vector.toarray() array([[0, 1, 1, 1, 0, 1, 0, 1, 1, 2, 0, 1], [1, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 2]]) <br>
					>>> cv.vocabulary_ {'basic': 0, 'beauty': 1, 'embedding': 2, 'into': 3, 'is': 4, 'look': 5, 'most': 6, 'of': 7, 'take': 8, 'the': 9, 'vectorizer': 10, 'word': 11}<br>
				</code>
				<br>
				<h3>Pros:</h3>
				<ul>
					<li>It is very easy and fast method to perform word embeddings.</li>
				</ul>
				<h3>Cons:</h3>
				<ul>
					<li>If the vocabulary is too large then the spase matrix created will be too large and will take a lot of memory. Also processing that huge matrix will be a burden.</li>
				</ul>
			</p>
			<p>
							<h2>2. Prediction Based Vector</h2>
							To overcome the limitations of previous methods of representation. Another method is introduced which with the help of 1-Hidden Layer Neural Network forms a N-dimensional representation of word called word vector.
							<br><br>
							These are great for many NLP task like word analogies and word similarities.
							<br>
							They can also perform task like King-Man+Woman = Queen
							<br><br>
							Lets take a look at 2 techniques to generate word vectors.
							<br>
							<h3>2.1  CBOW (Continuous Bag of Words)</h3>
							It works by finding or predicting the probability of a word in a given context. A context is a group of words. Given the context, we will predict the target word.
							<br><br>
							We will use neural network with 1-hidden layer whose size is equal to the size of word embedding we want.
							<br>
							Suppose, we have a vocabulary of size V, embedding size of N and context size of C.
							So architecture of neural network will be as follows:
							<br>
							<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04220606/Screenshot-from-2017-06-04-22-05-44-261x300.png" class="img-responsive" alt="Continuous Bag of Words Architecture">
							
							As shown in figure above, Input layer have multiple vectors given as input. These vectors are one hot encoded vectors. These multiple vectors belongs to each word in context.
							Hidden layer size is equal to embedding size. While output layer is a one hot encoded target word.
							<br><br>
							Objective function is Negative log likelihood of a word i.e. -log(p(wo/wi)) where,<br>
							wo : output words, wi : context words
							<br><br>
							Each word will be represented by a vector of size N i.e. Hidden Layer.

							<h3>2.2  Skip-Gram</h3>
							This is somewhat similar to CBOW, input is target word and the outputs are word surrounding target i.e. context. For example in sentence "I have a cute dog.". If input is "cute" then output is "I", "have", "a", "dog" assuming window size of 5.
							<br><br>
							Similar to CBOW, it contain 1 hidden layer of size equal to embedding size. 
							<br>
							<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/05000515/Capture2-276x300.png" class="img-responsive" alt="Skip-Gram architecture">
							As shown in figure above, Input layer have target one hot encoded work vector given as input. Hidden layer size is equal to embedding size. While output layer is a one hot encoded context words. 
							<br><br>
							Vectors are "meaningful" in terms of describing the relationship between words. The vector obtains by subtracting two related words sometimes express a meaningful concept such as gender or verb tense.
						</p>
						<p>
							<h3>Pros:</h3>
							<ul>
								<li>Word Vectors take less memory than previous word embedding methods.</li>
								<li>Word Vectors can be used to describe similarity between words using cosine similarity.</li>
								<li>Many library are already present like Gensim, Glove, Spacy which helps us to deal with Word vectors.</li>
							</ul>
							<h3>Cons:</h3>
							<ul>
								<li>Training for CBOW or Skip-Gram can take so much processing beacause of large vocabulary size.</li>
							</ul>
						</p>
						<p>
							Lets look at an example to build word vectors by using Gensim Library.
							<code>
								<br>
								import gensim.models.word2vec as w2v<br>
								import numpy as np<br>
								sentence_tokens = np.array([["This","is","a","game","of","thrones","books","corpus"],<br>
											["You","can","select","any","corpus"],<br>
											["You","must","convert","corpus","in","this","form"]])<br>

								embedding_size = 300 #size of embedding<br>
								min_word_count = 3 #word must appear atleast 3 times<br>
								num_workers = multiprocessing.cpu_count() #using multiple processors<br>
								context_size=7 #looking at 7 words at a time<br>
								downsampling = 1e-3 #Downsample setting for frequent words<br>
								<br>
								thrones2vec = w2v.Word2Vec(<br>
								    sg=1, #1 skip-gram 0- CBOW<br>
								    seed=seed,<br>
								    workers= num_workers,<br>
								    size = num_features,<br>
								    min_count = min_word_count,<br>
								    window = context_size,<br>
								    sample = downsampling<br>
								)<br>
								<br>
								thrones2vec.build_vocab(sentence_tokens)<br>
								#start training, this might take time<br>
								thrones2vec.train(sentence_tokens,<br>
								                  total_examples=len(sentence_tokens),<br>
								                  epochs=25<br>
								)<br>
								thrones2vec.save("thrones2vec.w2v") #to save word2vec model<br>
								thrones2vec = w2v.Word2Vec.load("thrones2vec.w2v") #to load word2vec model<br>
							</code>
							<br><br>
							Lets look at some applications of word2vec.
							<br>
							<code>
								>>> thrones2vec.wv.vectors #gives V*N dimensional matrix<br>
								>>> thrones2vec.wv.vocab #gives list of words of size V<br>
								>>> thrones2vec.wv.most_similar("stark")<br>
								[('eddard', 0.6009404063224792),<br>
								 ('snowbeard', 0.4654235243797302),<br>
								 ('accommodating', 0.46405118703842163),<br>
								 ('divulge', 0.4528692960739136),<br>
								 ('edrick', 0.43332362174987793),<br>
								 ('interred', 0.4253771901130676),<br>
								 ('executed', 0.42412883043289185),<br>
								 ('winterfell', 0.4224868416786194),<br>
								 ('shirei', 0.4207403063774109),<br>
								 ('absently', 0.419999361038208)]<br>
								>>> #Finding the degree of similarity between two words.<br>
								>>> thrones2vec.wv.similarity('woman','man')<br>
								0.73723527<br>
								>>> #Finding odd one out.<br>
								>>> thrones2vec.wv.doesnt_match('breakfast cereal dinner lunch';.split())<br>
								'cereal'<br>
								>>> #Amazing things like woman+king-man =queen<br>
								>>> thrones2vec.wv.most_similar(positive=['woman','king'],negative=['man'],topn=1)<br>
								queen: 0.508<br>
								>>> #Probability of a text under the model<br>
								>>> thrones2vec.wv.score(['The fox jumped over the lazy dog'.split()])<br>
								0.21<br>
								>>> def nearest_similarity_cosmul(start1, end1, end2):<br>
								.........similarities = thrones2vec.wv.most_similar_cosmul(<br>
								.........positive=[end2, start1],<br>
								.........negative=[end1])<br>
								.........start2 = similarities[0][0]<br>
								.........print("{start1} is related to {end1}, as {start2} is related to {end2}".format(**locals()))<br>
								>>> nearest_similarity_cosmul("stark", "winterfell", "riverrun")<br>
								'stark is related to winterfell, as tully is related to riverrun'<br>
								>>> nearest_similarity_cosmul("arya", "nymeria", "drogon")<br>
								'arya is related to nymeria, as dany is related to drogon'<br>
							</code>
						</p>
						<h2>More Resources</h2>
						<ol>
							<li><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank">Word Embedding tutorials by Analytics Vidhya</a></li>
							<li><a href="https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b" target="_blank">Skip Gram in detail by Towards Data Science</a></li>
						</ol>
		</div>
		<div id="disqus_thread"></div>			
    </div>
</section>
<section class="section-contact section-wrapper gray-bg">
    <div class="container-fluid">
        <div class="row">
            <div class="col-md-12">
                <div class="section-title">
                    <h2>Contact</h2>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <div class="feedback-form">
                    <h2>Get in touch</h2>

                    <form id="contactForm" action="https://formspree.io/adityajn105@gmail.com" method="POST" target='_blank'>
                        <div class="form-group">
                            <input type="hidden" name="_subject" value="Contact request from personal website" 
                            class="form-control" id="InputSubject">
                        </div>
                        <div class="form-group">
                            <label for="InputEmail">Email address</label>
                            <input type="email" name="_replyto" required="" class="form-control" id="InputEmail"
                                   placeholder="Email">
                        </div>
                        <div class="form-group">
                            <label for="message-text" class="control-label">Message</label>
                            <textarea class="form-control" rows="4" required="" name="message" id="message-text"
                                      placeholder="Write message"></textarea>
                        </div>

                        <button type="submit" class="btn btn-primary">Submit</button>
                    </form>
                </div>
            </div>
        </div>
    </div>
    <!--.container-fluid-->
</section>

<footer class="footer">
    <div class="copyright-section">
        <div class="container-fluid">
            <div class="row">
                <div class="col-md-12">
                    <div class="copytext">&copy; Resumex. All rights reserved | Design By: <a
                            href="https://themehippo.com">themehippo</a></div>
                </div>
            </div>
            <!--.row-->
        </div>
        <!-- .container-fluid -->
    </div>
    <!-- .copyright-section -->
</footer>
<!-- .footer -->
</div>
<!-- Sticky -->
</div>
<!-- .right-col-block -->
</div>
<!-- .columns-block -->
</div>
<!-- #main-wrapper -->

<!-- jquery -->
<script src="../js/jquery-2.1.4.min.js"></script>
<script type="text/javascript">
	$("#link").click(function() {
  $('html, body').animate({
    scrollTop: $("#disqus_thread").offset().top
  }, 1000);
});
</script>

<!-- Bootstrap -->
<script src="../js/bootstrap.min.js"></script>
<script src="../js/theia-sticky-sidebar.js"></script>
<script src="../js/scripts.js"></script>
</body>
</html>